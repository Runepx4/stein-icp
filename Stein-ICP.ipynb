{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import time\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pytorch3d.ops import knn_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%heat\n",
    "#device=\"cpu\"\n",
    "#dtype=torch.FloatTensor\n",
    "def squared_distance(x1, x2):\n",
    "    \"\"\"\n",
    "    Computes squared distance matrix between two arrays of row vectors.\n",
    "\n",
    "    Code originally from:\n",
    "        https://github.com/pytorch/pytorch/issues/15253#issuecomment-491467128\n",
    "    \"\"\"\n",
    "    \n",
    "    x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)\n",
    "    x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)\n",
    "    #x2_norm=x1_norm\n",
    "    #x2_norm[x1!=x2] = x2.pow(2).sum(dim=-1, keepdim=True)\n",
    "    \n",
    "      \n",
    "    res = torch.addmm(x2_norm.transpose(-2, -1), x1, x2.transpose(-2, -1), alpha=-2).add_(x1_norm)\n",
    "    return res\n",
    "   \n",
    "    \n",
    "def get_kernel(x, y, lengthscale=1.0):\n",
    "    pairwise_dists = squared_distance(x/lengthscale, y/lengthscale)\n",
    "    h_sq = 0.5 * torch.median(pairwise_dists) / math.log(x.shape[0])\n",
    "    return torch.exp(-0.5*pairwise_dists/h_sq)\n",
    "\n",
    "    \n",
    "def phi( X,score_func):\n",
    "    \"\"\"\n",
    "    \n",
    "    Code originally from:\n",
    "        https://colab.research.google.com/github/activatedgeek/stein-gradient/blob/master/Stein.ipynb#scrollTo=2fPfeNqDrYgo\n",
    "    \"\"\"\n",
    "    X = X.detach().requires_grad_(True)\n",
    "    K_XX = get_kernel(X, X.detach())\n",
    "    grad_K = -torch.autograd.grad(K_XX.sum(), X)[0]\n",
    "    phi = (K_XX.detach().matmul(score_func) + grad_K) / X.size(0)\n",
    "\n",
    "    return phi\n",
    "\n",
    "\n",
    "def plot_kde(x, y, z, roll, pitch, yaw, bwd, do_shading, max_dist):\n",
    "    def plot_one_dimension(data, label, color):\n",
    "        sns.kdeplot(\n",
    "            data.squeeze().cpu().detach().numpy(),\n",
    "            label=label,\n",
    "            color=color,\n",
    "            bw_adjust=bwd,\n",
    "            shade=do_shading\n",
    "        )\n",
    "        \n",
    "    _, ax = plt.subplots(1)\n",
    "    plot_one_dimension(x, \"X\", colors[0])\n",
    "    plot_one_dimension(y, \"Y\", colors[1])\n",
    "    plot_one_dimension(z, \"Z\", colors[2])\n",
    "    plot_one_dimension(roll, \"Roll\", colors[3])\n",
    "    plot_one_dimension(pitch, \"Pitch\", colors[4])\n",
    "    plot_one_dimension(yaw, \"Yaw\", colors[5])\n",
    "    ax.legend()\n",
    "\n",
    "    \n",
    "def plot_samples(x, y, z, roll, pitch, yaw):\n",
    "    def plot_one_dimension(data, label, color):\n",
    "        plt.plot(\n",
    "            data.squeeze().cpu().detach().numpy(),\n",
    "            label=label,\n",
    "            color=color,\n",
    "            linestyle=\"none\",\n",
    "            marker=\"o\"\n",
    "        )\n",
    "        \n",
    "    _, ax = plt.subplots(1)\n",
    "    plot_one_dimension(x, \"X\", colors[0])\n",
    "    plot_one_dimension(y, \"Y\", colors[1])\n",
    "    plot_one_dimension(z, \"Z\", colors[2])\n",
    "    plot_one_dimension(roll, \"Roll\", colors[3])\n",
    "    plot_one_dimension(pitch, \"Pitch\", colors[4])\n",
    "    plot_one_dimension(yaw, \"Yaw\", colors[5])\n",
    "    plt.xlabel(\"Particle #\")\n",
    "    plt.ylabel(\"Parameter Value\")\n",
    "    ax.legend()\n",
    "    \n",
    "\n",
    "def load_pcd(path_source, path_target, device):\n",
    "    \"\"\"Loads the provided source and target clouds.\"\"\"\n",
    "    pcd_s = o3d.io.read_point_cloud(path_source)\n",
    "    pcd_t = o3d.io.read_point_cloud(path_target)\n",
    "\n",
    "    s_arr = np.asarray(pcd_s.points)\n",
    "    t_arr = np.asarray(pcd_t.points)\n",
    "\n",
    "    s_tor = torch.from_numpy(s_arr).type(dtype).to(device)\n",
    "    t_tor = torch.from_numpy(t_arr).type(dtype).to(device)\n",
    "    \n",
    "    return s_tor, t_tor\n",
    "\n",
    "    \n",
    "def normalize_clouds(source_torch, target_torch):\n",
    "    \"\"\"Normalizes the cloud coordinates to lie withing [0, 1].\"\"\"\n",
    "    max_s = torch.max(torch.abs(source_torch))\n",
    "    #print(max_s)\n",
    "    max_t = torch.max(torch.abs(target_torch))\n",
    "    #print(max_t)\n",
    "    abs_max = torch.max(max_s, max_t)\n",
    "    source_torch = source_torch / abs_max\n",
    "    target_torch = target_torch / abs_max\n",
    "    \n",
    "    return source_torch, target_torch, abs_max\n",
    "\n",
    "\n",
    "def initialize_particles(particle_count, device, from_, to_):\n",
    "    b = 1\n",
    "    x =  (from_[0] - to_[0]) * torch.rand(particle_count, b, device=device) + to_[0]\n",
    "    y =  (from_[1] - to_[1]) * torch.rand(particle_count, b, device=device) + to_[1]\n",
    "    z =  (from_[2] - to_[2]) * torch.rand(particle_count, b, device=device) + to_[2] \n",
    "    r =  (from_[3] - to_[3]) * torch.rand(particle_count, b, device=device) + to_[3]\n",
    "    p =  (from_[4] - to_[4]) * torch.rand(particle_count, b, device=device) + to_[4] \n",
    "    ya = (from_[5] - to_[5]) * torch.rand(particle_count, b, device=device) + to_[5]\n",
    "\n",
    "    return x, y, z, r, p, ya    \n",
    "\n",
    "\n",
    "def get_stein_pairs(\n",
    "    source,             # particle_count x batchsize x 3\n",
    "    transformed_source, # particle_count x batchsize x 3\n",
    "    target,             # particle_count x batchsize x 3\n",
    "    max_dist            # threshold distance to reject matching pair\n",
    "):\n",
    "    \n",
    "    matches =  knn_points(transformed_source, target, None, None, K=1,return_nn=True)\n",
    "    # Get the target cloud at the paired indices\n",
    "    target_paired = matches.knn[:, :, 0, :]\n",
    "    \n",
    "    source=ignore_far_away_matches(source,matches.dists.squeeze() ,max_dist)\n",
    "    transformed_source=ignore_far_away_matches(transformed_source,matches.dists.squeeze() ,max_dist)\n",
    "    target_paired=ignore_far_away_matches(target_paired,matches.dists.squeeze() ,max_dist)\n",
    "    \n",
    "    return (source, transformed_source, target_paired)\n",
    "    \n",
    "\n",
    "\n",
    "#@torch.jit.script\n",
    "def ignore_far_away_matches(cloud,dist_norm,max_dist):\n",
    "    cloud[dist_norm > max_dist]=0\n",
    "    return cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%heat\n",
    "class SteinSGDICP:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        source_cloud,\n",
    "        target_cloud,\n",
    "        x, y, z, r, p, ya,\n",
    "        normalize_clouds=True,\n",
    "        iterations=150,\n",
    "        batch_size=150,\n",
    "        max_dist=1,\n",
    "        optimizer_class=\"Adam\",\n",
    "        lr=0.01\n",
    "    ):  \n",
    "        self.particle_size = x.shape[0]\n",
    "        \n",
    "        self.x = x.view(self.particle_size,1,1)\n",
    "        self.y = y.view(self.particle_size,1,1)\n",
    "        self.z = z.view(self.particle_size,1,1)\n",
    "        self.r = r.view(self.particle_size,1,1)\n",
    "        self.p = p.view(self.particle_size,1,1)\n",
    "        self.ya = ya.view(self.particle_size,1,1)\n",
    "     \n",
    "        self.device = target_cloud.device\n",
    "        \n",
    "        self.iter = iterations\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.max_dist = max_dist\n",
    "        \n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.source_cloud = source_cloud\n",
    "        self.target_cloud = target_cloud\n",
    "        self.normalize_clouds = normalize_clouds\n",
    "\n",
    "    def vectorized_rotation(self):\n",
    "        #r, p and ya are particle sizx1x1 column tensors where n is the number of particles\n",
    "        #output is particle sizex3x3\n",
    "        \n",
    "        T=torch.stack(\n",
    "            [torch.stack(\n",
    "                [ torch.cos(self.p)*torch.cos(self.ya), \n",
    "                 (torch.sin(self.r)*torch.sin(self.p)*torch.cos(self.ya)- torch.cos(self.r)*torch.sin(self.ya)), \n",
    "                 (torch.sin(self.r)*torch.sin(self.ya) + torch.cos(self.r)*torch.sin(self.p)*torch.cos(self.ya))\n",
    "                ],dim=3).squeeze(1),\n",
    "            torch.stack(\n",
    "                [torch.cos(self.p)*torch.sin(self.ya), \n",
    "                 (torch.cos(self.r)*torch.cos(self.ya) + torch.sin(self.r)*torch.sin(self.p)*torch.sin(self.ya)),\n",
    "                 (torch.cos(self.r)*torch.sin(self.p)*torch.sin(self.ya) -torch.sin(self.r)*torch.cos(self.ya))\n",
    "                ],dim=3).squeeze(1),\n",
    "            torch.stack(\n",
    "                [-torch.sin(self.p), \n",
    "                 torch.sin(self.r)*torch.cos(self.p), \n",
    "                 torch.cos(self.r)*torch.cos(self.p)\n",
    "                ],dim=3).squeeze(1)],\n",
    "            dim=2).squeeze()\n",
    "        \n",
    "        return T\n",
    "\n",
    "    def vectorized_trans(self):\n",
    "        \"\"\"Returns a tensor containing the translation parameters.\n",
    "        \n",
    "        Returns:\n",
    "            Returns a Nx3 tensor containing the translation parameters\n",
    "        \"\"\"\n",
    "        return torch.cat([self.x, self.y, self.z], dim=1)\n",
    "\n",
    "    def partial_derivatives_stein(self):\n",
    "        A = torch.cos(self.ya) \n",
    "        B = torch.sin(self.ya) \n",
    "        C = torch.cos(self.p) \n",
    "        D = torch.sin(self.p) \n",
    "        E = torch.cos(self.r) \n",
    "        F = torch.sin(self.r) \n",
    "\n",
    "        DE = D * E \n",
    "        DF = D * F \n",
    "        AC = A * C \n",
    "        AF = A * F \n",
    "        AE = A * E \n",
    "        ADE = A * DE \n",
    "\n",
    "        ADF = A * DF \n",
    "        BC = B * C \n",
    "        BE = B * E \n",
    "        BF = B * F \n",
    "        BDE = B * DE \n",
    "\n",
    "        torch_0 = torch.zeros(self.particle_size, 1, 1, device=self.device)\n",
    "        partial_roll = torch.stack([\n",
    "            torch.stack([torch_0,    ADE + BF,   BE - ADF],     dim=3).squeeze(1),\n",
    "            torch.stack([torch_0,   -AF + BDE,   B * -DF - AE], dim=3).squeeze(1),\n",
    "            torch.stack([torch_0,    C * E   ,   C * -F],       dim=3).squeeze(1)\n",
    "        ], dim=2).squeeze()\n",
    "\n",
    "        partial_pitch = torch.stack([\n",
    "                    torch.stack([A * -D, AC * F ,  AC * E], dim=3).squeeze(1),\n",
    "                    torch.stack([B * -D, BC * F , BC * E ], dim=3).squeeze(1),\n",
    "                    torch.stack([-C    , -DF    , -DE]    , dim=3).squeeze(1)\n",
    "        ], dim=2).squeeze()\n",
    "\n",
    "        partial_yaw = torch.stack([\n",
    "                    torch.stack([-BC,     -B * DF - AE ,  AF - BDE ], dim=3).squeeze(1),\n",
    "                    torch.stack([AC,      -BE + ADF    ,  ADE + BF ], dim=3).squeeze(1),\n",
    "                    torch.stack([torch_0,  torch_0    , torch_0    ], dim=3).squeeze(1)\n",
    "        ], dim=2).squeeze()\n",
    "\n",
    "        return partial_roll, partial_pitch, partial_yaw\n",
    "\n",
    "    def stein_grads(\n",
    "        self,\n",
    "        source_resized,   #each cloud here is particle_size x batchsize x 3\n",
    "        transformed_paired,\n",
    "        target_paired,\n",
    "        scaling_factor  # scaling with point size as required by stein formulation. \n",
    "    ):\n",
    "        # Get the number of dummy points (zero points inserted during filtering) to get\n",
    "        # mean by excluding their counts\n",
    "        zero_points_size = (transformed_paired.sum(dim=2)==0).sum(dim=1)\n",
    "        partial_derivatives = self.partial_derivatives_stein() \n",
    "\n",
    "        error = transformed_paired - target_paired\n",
    "\n",
    "        input_ = torch.empty(self.particle_size, 6, device=self.device) # particle_count  x 6\n",
    "        gradient_cost = torch.zeros_like(input_)\n",
    "\n",
    "        # For xyz gradients, get sum of error in xyz dim independently for each particle\n",
    "        # sumerr = error.sum(dim=1)\n",
    "        \n",
    "        # Zero matching will make grad to nan since currently we are not monitoring low\n",
    "        # matching pairs. So set it to 1 if we have number of zero=batch size (i.e. non matching pair)\n",
    "        a = torch.zeros_like(zero_points_size)\n",
    "        # set a to 1 where dummy points == batch size\n",
    "        a[(torch.where (zero_points_size==self.batch_size))] = 1\n",
    "\n",
    "        gradient_cost[:,0:3] = error.sum(dim=1)/((self.batch_size-zero_points_size.view(self.particle_size,1))+a.view(self.particle_size,1))#divide by no. of points after excluding dummy points\n",
    "        gradient_cost[:,3] = torch.einsum('pbc,pbc->pb',[error,(torch.einsum('prc,pbc->pbr', [(partial_derivatives)[0],source_resized]))]).sum(dim=1)/((self.batch_size-zero_points_size)+a)#.view(self.particle_size,1).squeeze())\n",
    "        gradient_cost[:,4] = torch.einsum('pbc,pbc->pb',[error,(torch.einsum('prc,pbc->pbr', [(partial_derivatives)[1],source_resized]))]).sum(dim=1)/((self.batch_size-zero_points_size)+a)#.view(self.particle_size,1).squeeze())\n",
    "        gradient_cost[:,5] = torch.einsum('pbc,pbc->pb',[error,(torch.einsum('prc,pbc->pbr', [(partial_derivatives)[2],source_resized]))]).sum(dim=1)/((self.batch_size-zero_points_size)+a)#.view(self.particle_size,1).squeeze())\n",
    "        \n",
    "        return gradient_cost*scaling_factor\n",
    "   \n",
    "    def stein_align(self):\n",
    "        abs_max=1\n",
    "        s_tor = self.source_cloud\n",
    "        t_tor = self.target_cloud\n",
    "        gradient_scaling_factor = s_tor.shape[0]\n",
    "        \n",
    "        if self.normalize_clouds:\n",
    "            s_tor, t_tor, abs_max = normalize_clouds(self.source_cloud, self.target_cloud)\n",
    "            self.x = self.x / abs_max\n",
    "            self.y = self.y / abs_max\n",
    "            self.z = self.z / abs_max\n",
    "            # If we normalize cloud, we need to scale gradients with normalizing factor to be stable\n",
    "            gradient_scaling_factor = abs_max * s_tor.shape[0]\n",
    "            \n",
    "        \n",
    "        def batch_generator():\n",
    "            while True:\n",
    "                indexes = np.random.randint(0, len(s_tor), self.batch_size)\n",
    "                yield s_tor[indexes]\n",
    "        mini_batch_sampler = batch_generator()\n",
    "     \n",
    "        parameters = [self.x, self.y, self.z, self.r, self.p, self.ya]\n",
    "        optimizer = None\n",
    "        print(\"Optimizing using: \", end=\"\")\n",
    "        if self.optimizer_class == \"Adam\":\n",
    "            print(\"Adam\")\n",
    "            optimizer = torch.optim.Adam(parameters, lr=self.lr)\n",
    "        elif self.optimizer_class == \"RMSprop\":\n",
    "            print(\"Prop\")\n",
    "            optimizer = torch.optim.RMSprop(\n",
    "                parameters,\n",
    "                lr=self.lr,\n",
    "                weight_decay=1e-8,\n",
    "                momentum=0.9\n",
    "            )\n",
    "        elif self.optimizer_class == \"SGD\":\n",
    "            print(\"SGD\")\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr)\n",
    "        elif self.optimizer_class == \"Adagrad\":\n",
    "            print(\"Adagrad\")\n",
    "            optimizer = torch.optim.Adagrad(parameters, lr=self.lr)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        max_dist = self.max_dist / abs_max\n",
    "\n",
    "        t_start = time.time()\n",
    "        \n",
    "        for epoch in range(self.iter):\n",
    "            # Get a minibatch \n",
    "            #minibatch = next(iter(mini_batch_sampler))\n",
    "            minibatch1 = next(mini_batch_sampler)\n",
    "            \n",
    "            minibatch = minibatch1.expand(self.particle_size, self.batch_size, 3).clone()\n",
    "\n",
    "            trans_mat = self.vectorized_trans()\n",
    "            rot_mat = self.vectorized_rotation()\n",
    "\n",
    "            s_transformed = torch.einsum(\n",
    "                \"pmc,prc->pmr\",\n",
    "                [minibatch,rot_mat]\n",
    "            ) + trans_mat.view(self.particle_size, 1, 3)\n",
    "            \n",
    "            \n",
    "            source_resized, transformed_paired, target_paired = get_stein_pairs(\n",
    "                    minibatch,\n",
    "                    s_transformed,\n",
    "                    t_tor.expand(self.particle_size, t_tor.shape[0], 3).clone(),\n",
    "                    max_dist\n",
    "                   \n",
    "                )\n",
    "\n",
    "\n",
    "            mean_grads = self.stein_grads(\n",
    "                source_resized,\n",
    "                transformed_paired,\n",
    "                target_paired,\n",
    "                gradient_scaling_factor\n",
    "            )\n",
    "\n",
    "            stein_v_grads = phi(\n",
    "                torch.stack([self.x,self.y,self.z,self.r,self.p,self.ya]).squeeze().T,\n",
    "                -mean_grads\n",
    "            )\n",
    "            #\n",
    "            self.x.grad = -stein_v_grads[:,0].view(self.particle_size, 1, 1).clone()\n",
    "            self.y.grad = -stein_v_grads[:,1].view(self.particle_size, 1, 1).clone()\n",
    "            self.z.grad = -stein_v_grads[:,2].view(self.particle_size, 1, 1).clone()\n",
    "\n",
    "            self.r.grad = -stein_v_grads[:,3].view(self.particle_size, 1, 1).clone()\n",
    "            self.p.grad = -stein_v_grads[:,4].view(self.particle_size, 1, 1).clone()\n",
    "            self.ya.grad = -stein_v_grads[:,5].view(self.particle_size, 1, 1).clone()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        t_end = time.time()\n",
    "        \n",
    "        particle_pose = torch.stack([\n",
    "            self.x*abs_max,\n",
    "            self.y*abs_max,\n",
    "            self.z*abs_max,\n",
    "            self.r,\n",
    "            self.p,\n",
    "            self.ya\n",
    "        ]).squeeze().T\n",
    "        \n",
    "        print(\"Stein ICP computation: {:.2f}s\".format(t_end - t_start))\n",
    "        return particle_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#device=\"cpu\"\n",
    "#dtype=torch.FloatTensor\n",
    "import torch\n",
    "particle_count = 100\n",
    "max_dist = 2.8\n",
    "path_source = \"source.pcd\"\n",
    "path_target = \"target.pcd\"\n",
    "\n",
    "#path_source=\"/home/fahira/code/dataset/laser_registration_dataset/gazebo_winter/local/pcd_clouds/1.pcd\"\n",
    "#path_target=\"/home/fahira/code/dataset/laser_registration_dataset/gazebo_winter/local/pcd_clouds/0.pcd\"\n",
    "\n",
    "from_ = torch.tensor([-0.8, -0.5, -0.1, -0.1753/4, -0.1753/4, -0.1753]).to(device)\n",
    "to_ = torch.tensor([0.8, 0.5, 0.1, 0.1753/4, 0.1753/4, 0.1753]).to(device)\n",
    "\n",
    "s_tor, t_tor = load_pcd(path_source, path_target, device)\n",
    "pose_guess=initialize_particles(particle_count, device, from_, to_)\n",
    "\n",
    "stein = SteinSGDICP(\n",
    "    s_tor,\n",
    "    t_tor,\n",
    "    pose_guess[0], pose_guess[1], pose_guess[2], pose_guess[3], pose_guess[4], pose_guess[5],\n",
    "    max_dist=max_dist,\n",
    "    optimizer_class=\"Adam\",\n",
    "    iterations=50,\n",
    "    lr=0.01,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "particle_poses = stein.stein_align()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde(\n",
    "    particle_poses[:, 0],\n",
    "    particle_poses[:, 1],\n",
    "    particle_poses[:, 2],\n",
    "    particle_poses[:, 3],\n",
    "    particle_poses[:, 4],\n",
    "    particle_poses[:, 5],\n",
    "    0.0254,\n",
    "    True,\n",
    "    max_dist\n",
    ")\n",
    "\n",
    "plot_samples(\n",
    "    particle_poses[:, 0],\n",
    "    particle_poses[:, 1],\n",
    "    particle_poses[:, 2],\n",
    "    particle_poses[:, 3],\n",
    "    particle_poses[:, 4],\n",
    "    particle_poses[:, 5]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
